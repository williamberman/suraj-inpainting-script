#!/bin/bash
#SBATCH --job-name=sdxl-inpaint-ema-2
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=96
#SBATCH --gres=gpu:8
#SBATCH --exclusive
#SBATCH --partition=production-cluster
#SBATCH --output=/admin/home/suraj/logs/maskgit-imagenet/%x-%j.out

set -x -e

source /admin/home/suraj/.bashrc
source /fsx/suraj/miniconda3/etc/profile.d/conda.sh
conda activate muse

echo "START TIME: $(date)"

REPO=/admin/home/suraj/code/muse-experiments/ctrlnet
OUTPUT_DIR=/fsx/suraj/sdxl-inpaint-ema-2
LOG_PATH=$OUTPUT_DIR/main_log.txt
ACCELERATE_CONFIG_FILE="$OUTPUT_DIR/${SLURM_JOB_ID}_accelerate_config.yaml.autogenerated"

mkdir -p $OUTPUT_DIR
touch $LOG_PATH
pushd $REPO


GPUS_PER_NODE=8
NNODES=$SLURM_NNODES
NUM_GPUS=$((GPUS_PER_NODE*SLURM_NNODES))

# so processes know who to talk to
MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
MASTER_PORT=6000


# Auto-generate the accelerate config
cat << EOT > $ACCELERATE_CONFIG_FILE
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MULTI_GPU
fsdp_config: {}
machine_rank: 0
main_process_ip: $MASTER_ADDR
main_process_port: $MASTER_PORT
main_training_function: main
num_machines: $SLURM_NNODES
num_processes: $NUM_GPUS
use_cpu: false
EOT


export MODEL_DIR="stabilityai/stable-diffusion-xl-base-1.0"
#--use_8bit_adam \
#--use_prodigy_optim  --use_cosine_annealing_schedule  \

PROGRAM="train_sdxl_inpaint.py \
    --pretrained_model_name_or_path=$MODEL_DIR \
    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \
    --output_dir=$OUTPUT_DIR \
    --mixed_precision=fp16 \
    --resolution=1024 \
    --learning_rate=4e-6 \
    --max_train_steps=50000 \
    --max_train_samples=3000000 \
    --dataloader_num_workers=8 \
    --validation_image=/admin/home/suraj/code/muse-experiments/inpaint-images  \
    --validation_prompt 'a man sitting on the bench' 'a tiger sitting on a bench' 'a dog in a park' \
    --train_shards_path_or_url='pipe:aws s3 cp s3://muse-datasets/laion-aesthetic6plus-min512-data/{00000..01210}.tar -' \
    --proportion_empty_prompts=0.05 \
    --validation_steps=500 \
    --train_batch_size=8 \
    --gradient_checkpointing \
    --enable_xformers_memory_efficient_attention \
    --gradient_accumulation_steps=1 \
    --seed=42 \
    --report_to=wandb \
    --resume_from_checkpoint=latest \
    --push_to_hub \
    --use_8bit_adam \
    --use_euler --use_ema \
    "

# Note: it is important to escape `$SLURM_PROCID` since we want the srun on each node to evaluate this variable
export LAUNCHER="accelerate launch \
    --rdzv_conf "rdzv_backend=c10d,rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT,max_restarts=0,tee=3" \
    --config_file $ACCELERATE_CONFIG_FILE \
    --main_process_ip $MASTER_ADDR \
    --main_process_port $MASTER_PORT \
    --num_processes $NUM_GPUS \
    --machine_rank \$SLURM_PROCID \
    "


export CMD="$LAUNCHER $PROGRAM"
echo $CMD

# hide duplicated errors using this hack - will be properly fixed in pt-1.12
# export TORCHELASTIC_ERROR_FILE=/tmp/torch-elastic-error.json

# force crashing on nccl issues like hanging broadcast
export NCCL_ASYNC_ERROR_HANDLING=1
# export NCCL_DEBUG=INFO
# export NCCL_DEBUG_SUBSYS=COLL
# export NCCL_SOCKET_NTHREADS=1
# export NCCL_NSOCKS_PERTHREAD=1
# export CUDA_LAUNCH_BLOCKING=1

# AWS specific
export NCCL_PROTO=simple
export RDMAV_FORK_SAFE=1
export FI_EFA_FORK_SAFE=1
export FI_EFA_USE_DEVICE_RDMA=1
export FI_PROVIDER=efa
export FI_LOG_LEVEL=1
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=ens


# srun error handling:
# --wait=60: wait 60 sec after the first task terminates before terminating all remaining tasks
# --kill-on-bad-exit=1: terminate a step if any task exits with a non-zero exit code
SRUN_ARGS=" \
    --wait=60 \
    --kill-on-bad-exit=1 \
    "

clear; srun $SRUN_ARGS --jobid $SLURM_JOB_ID bash -c "$CMD" 2>&1 | tee $LOG_PATH

echo "END TIME: $(date)"
